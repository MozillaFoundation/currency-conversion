{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About\n",
    "\n",
    "This is a version of adhoc_exchange_rates.ipynb that does not depend on the datautils submodule. It should only be used in a pinch, as it may not be up to date with the latest version of the functions from datautils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary libraries\n",
    "import json\n",
    "from ExchangeRateEvent import *\n",
    "from get_currencies import *\n",
    "from constants import API_URL, BASE_CURRENCY, LOCAL_CURRENCIES, SALESFORCE_INSTANCE_URL, SALESFORCE_API_USER, SALESFORCE_API_PASS, SALESFORCE_API_TOKEN, DB_URL, API_STATUS_CODES\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import timedelta, date, datetime\n",
    "import pandas as pd\n",
    "from file_opener import *\n",
    "from simple_salesforce import Salesforce, bulk2, format_soql\n",
    "from dateutil.parser import parse\n",
    "import urllib3\n",
    "from urllib3 import util\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from numpy import float64, int64, dtype, nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/gz/65b9jhcd3hsds0mhwvdrrbbc0000gn/T/ipykernel_20302/2150947264.py:67: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  search_object = re.compile('from\\s{1,}\\w{1,}(\\s{0,}|$)', re.IGNORECASE)\n",
      "/var/folders/gz/65b9jhcd3hsds0mhwvdrrbbc0000gn/T/ipykernel_20302/2150947264.py:70: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  replace = re.compile('(from|\\s)', re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "#define custom functions\n",
    "def file_opener(file, mode = 'r'):\n",
    "    if file:\n",
    "        open_file = open(file)\n",
    "        read_file = open_file.read()\n",
    "        open_file.close()\n",
    "    return read_file\n",
    "\n",
    "\n",
    "    #Salesforce reference of data types and the corresponding pandas dtype\n",
    "#https://developer.salesforce.com/docs/atlas.en-us.object_reference.meta/object_reference/field_types.htm\n",
    "DTYPE_MAPPER = {'objecting': 'object'\n",
    "                ,'double': 'float64'\n",
    "                ,'boolean': 'bool'\n",
    "                ,'currency': 'float64'\n",
    "                ,'textarea': 'object'\n",
    "                ,'date': 'date'\n",
    "                ,'datetime': 'datetime'\n",
    "                ,'id': 'object'\n",
    "                ,'masterrecord': 'object'\n",
    "                ,'reference': 'object'\n",
    "                ,'email': 'object'\n",
    "                ,'picklist': 'object'\n",
    "                ,'phone': 'object'\n",
    "                ,'percent': 'float64'\n",
    "                ,'location': 'object'\n",
    "                ,'address': 'object'\n",
    "                ,'string': 'object'\n",
    "                ,'url': 'object'}\n",
    "\n",
    "class sf_connect:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Create the tuple of environment variables to search for\n",
    "        env_vars = ('SALESFORCE_INSTANCE_URL'\n",
    "                    ,'SALESFORCE_API_USER'\n",
    "                    ,'SALESFORCE_API_PASS'\n",
    "                    ,'SALESFORCE_API_TOKEN')\n",
    "        \n",
    "        #Create an empty dict for the credentials\n",
    "        credentials = {}\n",
    "        \n",
    "        #Loop through the evironment variables and add them to the credential dict if they exist\n",
    "        #otherwise raise an exception.\n",
    "        for e in env_vars:\n",
    "            try:\n",
    "                credentials.update({e: os.environ[e]})\n",
    "            except:\n",
    "                raise Exception('Environment Variable {} does not exist, please set this value.'.format(e))\n",
    "        \n",
    "        #Set the credentials of the class\n",
    "        self.__credentials = credentials\n",
    "        \n",
    "        #Connect to Salesforce using the credentials from above\n",
    "        self.salesforce = Salesforce(instance = self.__credentials.get('SALESFORCE_INSTANCE_URL')\n",
    "                                    ,username = self.__credentials.get('SALESFORCE_API_USER')\n",
    "                                    ,password = self.__credentials.get('SALESFORCE_API_PASS')\n",
    "                                    ,security_token = self.__credentials.get('SALESFORCE_API_TOKEN')\n",
    "                                    ,session_id = requests.Session())\n",
    "        \n",
    "    def get_sf_object(self, query):\n",
    "        if query:\n",
    "            self.query = format_soql(query)\n",
    "\n",
    "            #Create the search expression: the \"from\" statement, with spaces and at least one of any letter or digit.\n",
    "            #The purpose is to extract the Salesforce Object\n",
    "            search_object = re.compile('from\\s{1,}\\w{1,}(\\s{0,}|$)', re.IGNORECASE)\n",
    "            \n",
    "            #Create the replacement expression, we want to replace the from and any space characters\n",
    "            replace = re.compile('(from|\\s)', re.IGNORECASE)\n",
    "\n",
    "            #Search the input query\n",
    "            result = re.search(search_object, self.query)\n",
    "\n",
    "            try:\n",
    "                #Replace the objectings to only get the object.\n",
    "                self.sf_object = re.sub(replace, '', result[0])\n",
    "                #Return the salesforce object.\n",
    "                #return sf_object\n",
    "            except:\n",
    "                raise Exception('No salesforce object found in query. The result is empty, please check your query')\n",
    "            \n",
    "    def sf_query_object(self, query):\n",
    "\n",
    "        #Add the salesforce object attribute from the query\n",
    "        self.get_sf_object(query)\n",
    "        \n",
    "        try:\n",
    "            #Create the connections to the appropriate endpoints\n",
    "            self.api_object = self.salesforce.__getattr__(self.sf_object)\n",
    "            self.bulk2_object = self.salesforce.bulk2.__getattr__(self.sf_object)\n",
    "\n",
    "            #Create a dict of column names and the salesforce data types\n",
    "            from_dtypes = {c.get('name'): c.get('type') for c in self.api_object.describe().get('fields')}\n",
    "    \n",
    "            #Connect to the object via bulk2\n",
    "            print('Querying data from Salesforce for the {} object...'.format(self.sf_object))\n",
    "            results = self.bulk2_object.query(self.query)\n",
    "            print('Query completed.')\n",
    "        #Otherwise raise any exceptions from the Salesforce class or otherwise\n",
    "        except Exception as e:\n",
    "            raise(e)\n",
    "\n",
    "        print('Parsing query results...')    \n",
    "        csv_data = [r for r in results]\n",
    "\n",
    "        df_list = []\n",
    "\n",
    "        #Iterate through the list of lists\n",
    "        for c in csv_data:\n",
    "            #Split each list by the newline characters\n",
    "            newline_split = c.split('\\n')\n",
    "            #Create a csv reader for all rows except the header, explicitly delimit by a comma\n",
    "            reader = csv.reader(newline_split[1:], delimiter = ',')\n",
    "            #Create a csv reader for only the header, explicitly delimit by a comma\n",
    "            col_reader = csv.reader([newline_split[0]], delimiter = ',')\n",
    "            #Append a dataframe with the values and columns to the df_list\n",
    "            df_list.append(pd.DataFrame([row for row in reader if row], columns = [c for c in col_reader]))\n",
    "        \n",
    "        if df_list:\n",
    "            print('Converting results to dataframe...')\n",
    "            #Concatenate the df_list\n",
    "            self.data = pd.concat(df_list)\n",
    "            #In some cases the columns may end up as a multi-index, reset them to just an index\n",
    "            self.data.columns = self.data.columns.get_level_values(0)\n",
    "            #Map these data types to the appropriate pandas data types\n",
    "            self.to_dtypes = {k: DTYPE_MAPPER.get(v) for k,v in from_dtypes.items() if k in self.data.columns}\n",
    "\n",
    "            #Create a dictionary of the existing dtypes\n",
    "            dtypes_dict = self.data.dtypes.apply(lambda x: x.name).to_dict()\n",
    "\n",
    "            #Loop through each column, compare the dtypes and change them if appropriate\n",
    "            print('Converting data types...')\n",
    "            for c in self.data.columns:               \n",
    "                to_dtype = self.to_dtypes.get(c)\n",
    "                #If the datatypes are not equal follow the specified procedures\n",
    "                #This currently fails for cross-referenced objects, like referencing Campaign.Name when quering CampaignMembers\n",
    "                if (to_dtype != dtypes_dict.get(c)) and (to_dtype is not None):\n",
    "                    print('Converting column {} to {}...'.format(c, to_dtype))   \n",
    "                    #If the to_dtype is a date then convert the column to a datetime.date\n",
    "                    if to_dtype == 'date':\n",
    "                        self.data[c] = self.data.apply(lambda x: datetime.strptime(x[c], '%Y-%m-%d').date(), axis = 1)\n",
    "                    #Else if the to_dtype is a datetime then convert the column to a datetime.datetime\n",
    "                    elif to_dtype == 'datetime':\n",
    "                        self.data[c] = self.data.apply(lambda x: datetime.strptime(x[c], '%Y-%m-%dT%H:%M:%S.%f%z'), axis = 1)\n",
    "                    #Else if the to_dtype is a boolean then convert the strings to a boolean\n",
    "                    elif to_dtype == 'bool':\n",
    "                        self.data[c] = self.data.apply(lambda x: True if x[c] == 'true' else (False if x[c] == 'false' else nan), axis = 1)\n",
    "                    #Else use the astype method for conversion as it functions the same for the other dtypes\n",
    "                    else:\n",
    "                        self.data[c] = self.data[c].astype(dtype(to_dtype))\n",
    "\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#set up parameters\u001b[39;00m\n\u001b[1;32m      2\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPI_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m file_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "#set up parameters\n",
    "API_KEY = os.environ['API_KEY']\n",
    "file_date = datetime.datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in SOQL query used to pull all relevant donations\n",
    "donation_soql = file_opener('opportunity.soql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the Salesforce API using the sf_connect.py file in the data_utils submodule\n",
    "sf = sf_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying data from Salesforce for the Opportunity object...\n",
      "Query completed.\n",
      "Parsing query results...\n",
      "Converting results to dataframe...\n",
      "Converting data types...\n",
      "Converting column Amount to float64...\n",
      "Converting column CloseDate to date...\n",
      "Converting column IsClosed to bool...\n",
      "Converting column IsWon to bool...\n",
      "Converting column Incorrect_Currency_Conversion__c to bool...\n",
      "Converting column Local_Currency_Amount_Number_For_Nat__c to float64...\n"
     ]
    }
   ],
   "source": [
    "#run the SOQL query using the API\n",
    "sf.sf_query_object(donation_soql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results of query as a local dataframe\n",
    "sf_donations_df = sf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a mapping to rename the Salesforce fields locally to match CurrencyAPI field names;\n",
    "renamer = {'Id': 'opportunity_id'\n",
    "           ,'Currency__c': 'local_currency'\n",
    "           ,'Local_Currency_Amount_Number_For_Nat__c': 'local_currency_amount'\n",
    "           ,'CloseDate': 'exchange_rate_date'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the field-renaming mapping to the dataframe, and save a list of unique donation dates locally (to help pull the minimum amount of data from the CurrencyAPI)\n",
    "if len(sf_donations_df) > 0:\n",
    "    sf_donations_df.rename(columns = renamer, inplace = True)\n",
    "    currencies_dates = sf_donations_df[['local_currency', 'exchange_rate_date']].copy().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Salesforce query returned a dataframe with 3164 records.\n"
     ]
    }
   ],
   "source": [
    "#print how many donations will need converting:\n",
    "print('The Salesforce query returned a dataframe with {} records.'.format(len(sf_donations_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the exchange rate for each donation from the Currency API using the get_currencies.py file\n",
    "\n",
    "if len(currencies_dates) > 0:\n",
    "    records = []\n",
    "    for c in currencies_dates.iterrows():\n",
    "        data = c[1]\n",
    "        local_currency = data[0]\n",
    "        date = data[1].strftime('%Y%m%d')\n",
    "\n",
    "        exchange_rate = get_currencies(\n",
    "                                       BASE_CURRENCY\n",
    "                                       ,local_currency\n",
    "                                       ,date\n",
    "                                       ,API_KEY\n",
    "                                       ,API_URL\n",
    "                                       )\n",
    "        records.append(exchange_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "data = [rows.__dict__ for days in records for rows in days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format results as a dataframe\n",
    "if len(data) > 0:\n",
    "    all_exchange_rates = pd.DataFrame(data)\n",
    "    all_exchange_rates['exchange_rate_date'] = all_exchange_rates.apply(lambda x: parse(x['exchange_rate_date']).date(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results as a CSV\n",
    "if len(all_exchange_rates) > 0 and len(sf_donations_df) > 0:\n",
    "    conversion = sf_donations_df.merge(all_exchange_rates[['local_currency', 'exchange_rate_date', 'exchange_rate']]\n",
    "                                       ,how = 'left'\n",
    "                                       ,on = ['local_currency', 'exchange_rate_date']\n",
    "                                       ,indicator = True)\n",
    "    \n",
    "    conversion['base_currency_amount'] = conversion.apply(lambda x: round(x['local_currency_amount']/x['exchange_rate'], 2), axis = 1)\n",
    "    \n",
    "    print('{} records were successfuly converted.'.format(len(conversion[conversion['_merge'] == 'both'])))\n",
    "    \n",
    "    conversion[conversion['_merge'] == 'both'].to_csv('currency_conversion_{}.csv'.format(file_date))\n",
    "    \n",
    "    #conversion[conversion['_merge'] != 'both'].to_csv('no_currency_conversion_{}.csv'.format(file_date))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
